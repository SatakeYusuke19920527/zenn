---
title: "ã€Let's ãƒãƒ³ã‚ºã‚ªãƒ³ğŸ–ï¸ã€‘Next.jsã¨Azure AI Serviceã§ä½œã‚‹ è‹±ä¼šè©±Lessonã‚¢ãƒ—ãƒª ğŸš€"
emoji: "ğŸ§‘â€ğŸ«"
type: "tech" # tech: æŠ€è¡“è¨˜äº‹ / idea: ã‚¢ã‚¤ãƒ‡ã‚¢
topics: ["Next.js", "Azure", "OpenAI","Microsoft", "AzureAISpeech"]
published: false
---

![Azure](/images/azure_next_english_lesson/img1.png)

# ã¯ã˜ã‚ã«

ã“ã®è¨˜äº‹ã§ã¯ã€Next.js ã¨ Azure AI Service ã‚’ä½¿ã£ã¦ã€è‹±ä¼šè©±ã®ãƒ¬ãƒƒã‚¹ãƒ³ã‚¢ãƒ—ãƒªã‚’ä½œæˆã—ã¾ã™ã€‚
ãƒ•ãƒ­ãƒ³ãƒˆéƒ¨åˆ†ã®ã‚³ãƒ¼ãƒ‰ã¯ã€Next.js ã§ä½œæˆã—ã€ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰éƒ¨åˆ†ã¯ Azure AI Service ã‚’ä½¿ã£ã¦ãƒãƒ³ã‚ºã‚ªãƒ³ã§å®Ÿè£…ã—ã¾ã™ã€‚
çš†æ§˜ã‚‚æ˜¯éå®Ÿè£…ã—ã¦ã¿ã¦ãã ã•ã„ã€‚

ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£å›³ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚
![Azure](/images/azure_next_english_lesson/img15.png)

1. Next.jsã§å®Ÿè£…ã—ãŸãƒ–ãƒ©ã‚¦ã‚¶ã‚ˆã‚ŠAzure AI Speechã§è³ªå•ã‚’éŸ³å£°å…¥åŠ›ã—ã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§éŸ³å£°â†’ãƒ†ã‚­ã‚¹ãƒˆã¸å¤‰æ›
2. ãƒ†ã‚­ã‚¹ãƒˆã¸å¤‰æ›ã—ãŸè³ªå•ã‚’Azure OpenAIã¸æŠ•ã’ã€å›ç­”ã‚’ç”Ÿæˆ
3. ç”Ÿæˆã—ãŸå›ç­”ã‚’Azure AI Speechã§ãƒ†ã‚­ã‚¹ãƒˆâ†’éŸ³å£°å¤‰æ›ã—ã€ãƒ–ãƒ©ã‚¦ã‚¶ã«è¿”å´
4. ãƒ–ãƒ©ã‚¦ã‚¶ã§éŸ³å£°ã‚’å†ç”Ÿ

æ¥½ã—ãã†ã§ã™ã‚ˆã­ã€‚

å®Ÿéš›ã®ç”»é¢ã¯ä»¥ä¸‹ã®ã‚ˆã†ãªã‚·ãƒ³ãƒ—ãƒ«ãªç”»é¢ã§ã™ã€‚
![Azure](/images/azure_next_english_lesson/img3.png)

ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ã€
Hi, How about you?
ã¨è¨€ã†ã¨ã€OpenAIãŒéŸ³å£°ã§
Hello! I'm here and ready to assist you. How can I help you today? .
ã¨ã„ã„æ„Ÿã˜ã«è¿”ç­”ã—ã¦ãã‚Œã¾ã™ã€‚

ãã‚Œã§ã¯ã‚„ã£ã¦ã¿ã¾ã—ã‚‡ã†ğŸš€

# Next.jsã§ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã®ã‚¢ãƒ—ãƒªã‚’å®Ÿè£…

## ç’°å¢ƒæ§‹ç¯‰
ãã‚Œã§ã¯Next.jsã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‹ã‚‰å§‹ã‚ã¾ã—ã‚‡ã†ã€‚
ä»»æ„ã®ãƒ•ã‚©ãƒ«ãƒ€ã«ã¦ã€ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚

```bash
npx create-next-app . --ts
```

è¨­å®šã¯å…¨éƒ¨Yesã§å›ç­”ã§OKã§ã™ã€‚

```bash
âœ” Would you like to use ESLint with this project? â€¦ No / Yes
âœ” Would you like to use Tailwind CSS with this project? â€¦ No / Yes
âœ” Would you like to use `src/` directory with this project? â€¦ No / Yes
âœ” Use App Router (recommended)? â€¦ No / Yes
âœ” Would you like to customize the default import alias? â€¦ No / Yes
âœ” What import alias would you like configured? â€¦ @/*
Creating a new Next.js app in /Users/s.y/prj/04.webapp/sat-web-repo.
```

Next.jsã®templateãŒä½œæˆã•ã‚ŒãŸã¨æ€ã„ã¾ã™ã€‚

## ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

ä»Šå›ã® package.json ã¯ã“ã‚“ãªæ„Ÿã˜
ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’åˆã‚ã›ã‚‹ç‚ºã€ã”è‡ªèº«ã® dependencies ã®ç®‡æ‰€ã‚’ä¸‹è¨˜ã®dependencies ã¸ä¸Šæ›¸ãã—ã¦ãã ã•ã„ã€‚

```bash
{
  "name": "appå",
  "version": "0.1.0",
  "private": true,
  "scripts": {
    "dev": "next dev",
    "build": "next build",
    "start": "next start",
    "lint": "next lint"
  },
  "dependencies": {
    "@azure/openai": "^1.0.0-beta.12",
    "@types/node": "22.5.0",
    "@types/react": "18.3.4",
    "@types/react-dom": "18.3.0",
    "autoprefixer": "10.4.20",
    "axios": "^1.7.5",
    "fs": "^0.0.1-security",
    "microsoft-cognitiveservices-speech-sdk": "^1.40.0",
    "next": "14.2.6",
    "node-wav-player": "^1.0.0",
    "postcss": "8.4.41",
    "react": "18.3.1",
    "react-dom": "18.3.1",
    "tailwindcss": "3.4.10",
    "typescript": "5.5.4",
    "universal-cookie": "^7.2.0",
    "use-sound": "^4.0.3"
  }
}

```

ãã—ã¦ã€å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä¸€æ°—ã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚
```bash
npm install
```

ã“ã‚Œã§ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãŒå®Œäº†ã§ã™ã€‚

## å®Ÿè£…
æœ€çµ‚çš„ãª src/ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®é…ä¸‹ã¯ä»¥ä¸‹ã®ã‚ˆã†ãªå¾Œæˆã«ãªã‚Šã¾ã™ã€‚

![Azure](/images/azure_next_english_lesson/img4.png)

ã§ã¯ã€ä½œã£ã¦ã„ãã¾ã—ã‚‡ã†

ã¾ãšã¯ãƒ«ãƒ¼ãƒˆéšå±¤ã¸.env.localãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã€ä»¥ä¸‹ã®å†…å®¹ã‚’è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚

```
SPEECH_KEY=xxxxxxxxxxxxxxxxxxxxxxxxxxxxx
SPEECH_REGION=japaneast
NEXT_PUBLIC_URL=http://localhost:3000
AZURE_OPENAI_ENDPOINT=https://xxxxxxxxxxxxxxxxxxxxxxxxxxxxx.com/
AZURE_OPENAI_API_KEY=xxxxxxxxxxxxxxxxxxxxxxxxxxxxx
AZURE_OPENAI_DEPLOYMENT_ID=gpt-4o
```
srcé…ä¸‹ã« types, utilãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

![Azure](/images/azure_next_english_lesson/img5.png)

util/token_util.tsãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

```typescript
import axios from 'axios';
import Cookie from 'universal-cookie';

export async function getTokenOrRefresh() {
    const cookie = new Cookie();
    const speechToken = cookie.get('speech-token');

    if (speechToken === undefined) {
        try {
            const res = await axios.get('/api/get-speech-token');
            const token = res.data.token;
            const region = res.data.region;
            cookie.set('speech-token', region + ':' + token, {maxAge: 540, path: '/'});
            return { authToken: token, region: region };
        } catch (err: any) {
            console.log(err.response.data);
            return { authToken: null, error: err.response.data };
        }
    } else {
        const idx = speechToken.indexOf(':');
        return { authToken: speechToken.slice(idx + 1), region: speechToken.slice(0, idx) };
    }
}
```

æ¬¡ã«types/samplevoice.d.tsãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

```typescript
// src/types/custom.d.ts
declare module '*.wav' {
  const value: string;
  export default value;
}
```

æ¬¡ã«ã€pages/app/pages.tsxãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿®æ­£ã—ã¾ã™ã€‚

```typescript
'use client';
import { ResultReason } from 'microsoft-cognitiveservices-speech-sdk';
import { useState } from 'react';
import { getTokenOrRefresh } from '../util/token_util';

const speechsdk = require('microsoft-cognitiveservices-speech-sdk');

export default function Home() {
  const [isLoading, setIsLoading] = useState<boolean>(false);
  const [displayText, setDisplayText] = useState("");
  const [displayAIText, setDisplayAIText] = useState('');
  const [player, updatePlayer] = useState({ p: undefined, muted: false });

  async function sttFromMic() {
    setIsLoading(true);
    const tokenObj = await getTokenOrRefresh();
    const speechConfig = speechsdk.SpeechConfig.fromAuthorizationToken(
      tokenObj.authToken,
      tokenObj.region
    );
    speechConfig.speechRecognitionLanguage = 'en-US';

    const audioConfig = speechsdk.AudioConfig.fromDefaultMicrophoneInput();
    const recognizer = new speechsdk.SpeechRecognizer(
      speechConfig,
      audioConfig
    );

    setDisplayText('speak into your microphone...');

    await recognizer.recognizeOnceAsync((result: any) => {
      if (result.reason === ResultReason.RecognizedSpeech) {
        setDisplayText(`You: ${result.text}`);
        getAzData(result.text);
      } else {
        setDisplayText(
          'ERROR: Speech was cancelled or could not be recognized. Ensure your microphone is working properly.'
        );
      }
    });
    setIsLoading(false);
  }

  const getAzData = async (message: string) => {
    setIsLoading(true);
    try {
      console.log('start message : ', message);
      // const url = '/api/azopenai';

      const response = await fetch(`http://localhost:3000/api/azopenai`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({ message }),
      });

      console.log('ğŸš€ ~ getAzData ~ response:', response);
      const { aiMessage } = await response.json();
      console.log('ğŸš€ ~ getAzData ~ aiMessage:', aiMessage[0].message.content);
      // å›ç­”ã‚’éŸ³å£°ã§èª­ã¿ä¸Šã’
      textToSpeech(aiMessage[0].message.content);
    } catch (err) {
      console.log('ğŸš€ ~ file: index.tsx:32 ~ getAzData ~ err:', err);
    }
    setIsLoading(false);
  };

  const textToSpeech = async(answer: string) => {
    const tokenObj = await getTokenOrRefresh();
    const speechConfig = speechsdk.SpeechConfig.fromAuthorizationToken(
      tokenObj.authToken,
      tokenObj.region
    );
    const myPlayer = new speechsdk.SpeakerAudioDestination();
    updatePlayer((p) => {
      p.p = myPlayer;
      return p;
    });
    const audioConfig = speechsdk.AudioConfig.fromSpeakerOutput(player.p);

    let synthesizer = new speechsdk.SpeechSynthesizer(
      speechConfig,
      audioConfig
    );

    const textToSpeak = answer;
    // setDisplayText();
    setDisplayAIText(`AI : ${textToSpeak}`);
    synthesizer.speakTextAsync(
      textToSpeak,
      (result: any) => {
        let text;
        if (
          result.reason === speechsdk.ResultReason.SynthesizingAudioCompleted
        ) {
          text = `${textToSpeak} .\n`;
        } else if (result.reason === speechsdk.ResultReason.Canceled) {
          text = `synthesis failed. Error detail: ${result.errorDetails}.\n`;
        }
        synthesizer.close();
        synthesizer = undefined;
        setDisplayAIText("AI : " + text!);
      },
      function (err: any) {
        setDisplayAIText(`Error: ${err}.\n`);

        synthesizer.close();
        synthesizer = undefined;
      }
    );
  }
  
  return (
    <div className="app-container p-4">
      <h1 className="display-4 mb-5 text-4xl">Speech talk with AI</h1>
      <div className="row main-container">
        <div className="col-6">
          <button
            type="button"
            onClick={() => sttFromMic()}
            className="text-white bg-blue-700 hover:bg-blue-800 focus:ring-4 focus:ring-blue-300 font-medium rounded-lg text-sm px-5 py-2.5 me-2 mb-2 dark:bg-blue-600 dark:hover:bg-blue-700 focus:outline-none dark:focus:ring-blue-800"
          >
            talk to AI
          </button>
        </div>
        <hr className="my-5" />
        <div className="p-4 col-6 output-display rounded">
          <div className="mb-5 text-gray-500">{displayText}</div>
          <div className="mb-5 text-gray-500">{displayAIText}</div>
        </div>
        {isLoading ? (
          <div className="flex justify-center" aria-label="èª­ã¿è¾¼ã¿ä¸­">
            <div className="animate-spin h-8 w-8 bg-blue-300 rounded-xl"></div>
          </div>
        ) : (
          <></>
        )}
      </div>
    </div>
  );
}

```

æ¬¡ã«appé…ä¸‹ã¸apiãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆã—ã€app/api/é…ä¸‹ã‚’ä»¥ä¸‹ã®ã‚ˆã†ã«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

![Azure](/images/azure_next_english_lesson/img6.png)

azopenai/route.tsãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å®Ÿè£…ã—ã¾ã™ã€‚

```typescript
import { AzureKeyCredential, OpenAIClient } from '@azure/openai';
import { NextRequest } from 'next/dist/server/web/spec-extension/request';
import { NextResponse } from 'next/dist/server/web/spec-extension/response';

export const POST = async (req: NextRequest) => {
  try {
    const { message } = await req.json();

    const aiMessage = await returnAiMessage(message);
    console.log("ğŸš€ ~ POST ~ aiMessage:", aiMessage)

    return NextResponse.json({ aiMessage }, { status: 200 });
  } catch (error: any) {
    return NextResponse.json({ aiMessage: error.message }, { status: 500 });
  }
};

const returnAiMessage = (message: any) => {
  return new Promise(async (resolve, reject) => {
    const endpoint = process.env.AZURE_OPENAI_ENDPOINT!;
    const azureApiKey = process.env.AZURE_OPENAI_API_KEY!;
    const deploymentId = process.env.AZURE_OPENAI_DEPLOYMENT_ID!;
    const content = `
      ${message}
      `;
    try {
      const messages = [
        { role: 'system', content: 'You are a helpful assistant.' },
        {
          role: 'user',
          content,
        },
      ];
      const client = new OpenAIClient(
        endpoint,
        new AzureKeyCredential(azureApiKey)
      );

      const result = await client.getChatCompletions(deploymentId, messages);
      console.log("ğŸš€ ~ answer ~ result:", result.choices)
      resolve(result.choices);
    } catch (error: any) {
      console.log(
        'ğŸš€ ~ file: openaiRepository.ts:29 ~ AzOpenaiRepository ~ returnnewPromise ~ error:',
        error
      );
      reject(error);
    }
  });
}

export const dynamic = 'force-dynamic';

```

æ¬¡ã«get-speech-token/route.tsãƒ•ã‚¡ã‚¤ãƒ«ã‚’å®Ÿè£…ã—ã¾ã™ã€‚

```typescript
import axios from 'axios';
import { NextRequest } from 'next/dist/server/web/spec-extension/request';
import { NextResponse } from 'next/dist/server/web/spec-extension/response';

export const GET = async (res:NextResponse,req: NextRequest) => {  
  try {
        const speechKey = process.env.SPEECH_KEY;
        console.log("ğŸš€ ~ GET ~ speechKey:", speechKey)
        const speechRegion = process.env.SPEECH_REGION;
        
        const headers = {
          headers: {
            'Ocp-Apim-Subscription-Key': speechKey,
            'Content-Type': 'application/x-www-form-urlencoded',
          },
        };

      try {
        const tokenResponse = await axios.post(
          `https://${speechRegion}.api.cognitive.microsoft.com/sts/v1.0/issueToken`,
          null,
          headers
        );
        // res.send({ token: tokenResponse.data, region: speechRegion });
        return NextResponse.json(
          {token: tokenResponse.data, region: speechRegion },
          { status: 200 }
        );
      } catch (err) {
        return NextResponse.json(
          { aiMessage: 'test message' },
          { status: 401 }
        );
      }
  } catch (error: any) {
    return NextResponse.json({ aiMessage: error.message }, { status: 500 });
  }
};

export const dynamic = 'force-dynamic';

```

ã“ã‚Œã§Next.jsã®å®Ÿè£…ã¯å®Œäº†ã§ã™ã€‚
ã‚¢ãƒ—ãƒªã‚’èµ·å‹•ã—ã¦ã€ã‚¨ãƒ©ãƒ¼ãŒå‡ºãªã‘ã‚Œã°OKã§ã™ã€‚

```bash
npm run dev
```

# Azure ãƒªã‚½ãƒ¼ã‚¹ã®Deploy

## ãƒªã‚½ãƒ¼ã‚¹ã‚°ãƒ«ãƒ¼ãƒ—ã®ä½œæˆ
ã¾ãšã¯ã€Azure Portalã«ãƒ­ã‚°ã‚¤ãƒ³ã—ã¦ãƒªã‚½ãƒ¼ã‚¹ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

![Azure](/images/azure_next_english_lesson/img7.png)

ç”»é¢ä¸Šéƒ¨ã®çª“ã‚ˆã‚Š ```ãƒªã‚½ãƒ¼ã‚¹ã‚°ãƒ«ãƒ¼ãƒ—``` ã‚’æ¤œç´¢ã—ã€ãƒªã‚½ãƒ¼ã‚¹ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

![Azure](/images/azure_next_english_lesson/img8.png)

## OpenAI Serviceã®ä½œæˆ

ã§ã¯æ¬¡ã¯OpenAI Serviceã‚’ä½œæˆã—ã¾ã™ã€‚
Azure Portalã®å·¦ä¸Šã®çª“ã‚ˆã‚Š ```OpenAI``` ã‚’æ¤œç´¢ã—ã€OpenAIã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚

ãã—ã¦ä»¥ä¸‹ã®ã‚ˆã†ã«ãƒªã‚½ãƒ¼ã‚¹ã‚’Deloyã—ã¦ãã ã•ã„ã€‚
![Azure](/images/azure_next_english_lesson/img9.png)

æ¬¡ã¯Azure OpenAI Studioã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦ã€ãƒ¢ãƒ‡ãƒ«ã®Deployã‚’è¡Œã£ã¦ãã ã•ã„

![Azure](/images/azure_next_english_lesson/img10.png)

ã“ã‚Œã§OpenAIã®DeployãŒå®Œäº†ã§ã™ã€‚

## Azure AI Speech ã®ä½œæˆ
æ¬¡ã«Azure AI Speechã‚’ä½œæˆã—ã¾ã™ã€‚
Azure Portalã®æ¤œç´¢çª“ã‚ˆã‚Š ```éŸ³å£°ã‚µãƒ¼ãƒ“ã‚¹``` ã‚’æ¤œç´¢ã—ã€éŸ³å£°ã‚µãƒ¼ãƒ“ã‚¹ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚

![Azure](/images/azure_next_english_lesson/img11.png)

Deployå‡ºæ¥ã‚Œã°OKã§ã™ï¼

# ç’°å¢ƒå¤‰æ•°ã®è¨­å®š
.env.local ã®ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®šã—ã¾ã™ã€‚

Azure OpenAI Serviceã¯ä»¥ä¸‹ã‚’å‚ç…§ã§ã™ã€‚
ãƒªã‚½ãƒ¼ã‚¹ç®¡ç† > ã‚­ãƒ¼ã¨ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
![Azure](/images/azure_next_english_lesson/img12.png)

éŸ³å£°ã‚µãƒ¼ãƒ“ã‚¹ã‚‚åŒæ§˜ã§ã™ã€‚
ãƒªã‚½ãƒ¼ã‚¹ç®¡ç† > ã‚­ãƒ¼ã¨ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
![Azure](/images/azure_next_english_lesson/img13.png)

ä»¥ä¸‹ã®ã‚ˆã†ãªæ„Ÿã˜ã«ãªã‚Šã¾ã™ã€‚
```
SPEECH_KEY=xxxxxxxxxxxxxxxxxxxxxxxxxx
SPEECH_REGION=japaneast
NEXT_PUBLIC_URL=http://localhost:3000

AZURE_OPENAI_ENDPOINT=https://xxxxxx.openai.azure.com/
AZURE_OPENAI_API_KEY=xxxxxxxxxxxxxxxxxxxxxxxxxx
AZURE_OPENAI_DEPLOYMENT_ID=gpt-4o
```
ã“ã‚Œã§ç’°å¢ƒå¤‰æ•°ã®è¨­å®šã¯å®Œäº†ã§ã™!

# å‹•ä½œç¢ºèª

ã§ã¯ã‚¢ãƒ—ãƒªã‚’èµ·å‹•ã—ã¦ã¿ã¾ã—ã‚‡ã†ï¼
![Azure](/images/azure_next_english_lesson/img13.png)

Buttonã‚’æŠ¼ã—ã¦ã€éŸ³å£°ã§è³ªå•ã‚’å…¥åŠ›ã—ã¦ã¿ã¦ãã ã•ã„ã€‚
OpenAIã‹ã‚‰ã®å›ç­”ãŒéŸ³å£°ã§å¸°ã£ã¦ãã‚Œã°æˆåŠŸã§ã™ï¼

ãŠç–²ã‚Œæ§˜ã§ã—ãŸï¼


# æœ€å¾Œã«
ã„ã‹ãŒã ã£ãŸã§ã—ã‚‡ã†ã‹ã€‚
ä»Šå›ã¯ã€Next.jsã¨Azure AI Serviceã‚’ä½¿ã£ã¦ã€è‹±ä¼šè©±ã®ãƒ¬ãƒƒã‚¹ãƒ³ã‚¢ãƒ—ãƒªã‚’ä½œæˆã—ã¾ã—ãŸã€‚

éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã¸å¤‰æ›ã—ã€OpenAIã§å›ç­”ä½œæˆã—ã¦ã€æœ€å¾Œã«ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã¸å¤‰æ›ã—ã¦è¿”å´ã™ã‚‹ã¨ã„ã†æµã‚Œã§ã—ãŸã€‚

Azure AI Serviceã¯ã€ä¸Šè¨˜ä»¥å¤–ã«ã‚‚æ§˜ã€…ãªã‚µãƒ¼ãƒ“ã‚¹ãŒæä¾›ã•ã‚Œã¦ã„ã¾ã™ã€‚

çš†æ§˜ã‚‚æ˜¯éæ´»ç”¨ã—ã¦é¢ç™½ã„ã‚¢ãƒ—ãƒªã‚’ä½œã£ã¦ã¿ã¦ãã ã•ã„ã€‚

ãã‚Œã§ã¯ğŸ–ï¸

# å‚è€ƒè³‡æ–™
https://learn.microsoft.com/ja-jp/azure/ai-services/speech-service/overview
