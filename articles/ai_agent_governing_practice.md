---
title: 'AI エージェントシステムのガバナンス実践ガイド'
emoji: '📃'
type: 'tech' # tech: 技術記事 / idea: アイデア
topics: ['AIagent', 'Governance', 'Governance', 'OpenAI', 'Microsoft']
published: false
publication_name: microsoft
---

![](https://storage.googleapis.com/zenn-user-upload/1f5fc7603895-20250721.png)

# はじめに

**本記事は「自律的に複雑な目標を追求する AI Agent システム」を 安全かつ責任ある形で社会実装するための ガバナンス実践ガイドです。**

AI エージェントシステムは限られた直接監督の下で複雑な目標を遂行でき、社会に責任ある形で統合できれば広く有用になると考えられます。
しかし、AI エージェントシステムは、人々が自らの目標をより効率的かつ効果的に達成する手助けとなる大きな可能性を秘めていますが、同時に危害をもたらすリスクも存在します。

本記事では、OpenAI が 2024 年に公開したホワイトペーパー

**“Practices for Governing Agentic AI Systems”**

を参考にしながら、AI エージェントシステムと、そのライフサイクルに関わる当事者を定義し、それぞれが合意すべき基本的な責務と安全性のベストプラクティスの重要性を記載します。
そして、AI エージェントの運用を安全かつ説明可能に保つための初期的な実践手法を提示します。

https://cdn.openai.com/papers/practices-for-governing-agentic-ai-systems.pdf

筆者は AI エージェントシステムを作って終わり、ではなく、その先の運用とガバナンス、責務の明示と安全性の担保をすることが、より一層 AI エージェントの社会実装を進める為に重要と考えています。

本記事でこれから AI エージェントシステムを作る方々が悩めるガバナンスの観点について有意義な資料となるように記載してみました。

みなさんの AI エージェントシステムの開発と運用がより一層社会にとって有意義で安全なものになることを祈り、内容に入りたいと思います。

# 目次

1. **導入**
2. **定義**
   2.1 エージェント性・AI エージェントシステム・「エージェント」
   2.2 AI エージェントのライフサイクルにおける人間の当事者
3. **AI エージェントシステムの潜在的利点**
   3.1 有用な特性としてのエージェント性
   3.2 影響力を増幅するエージェント性
4. **AI エージェントシステムを安全かつ説明可能に保つ実践策**
   4.1 タスク適合性の評価
   4.2 行動範囲の制約と承認フロー
   4.3 エージェントのデフォルト行動の設定
   4.4 エージェント活動の可視化（レジビリティ）
   4.5 自動監視
   4.6 責任帰属（アトリビュタビリティ）
   4.7 中断可能性と制御維持
5. **AI エージェントシステム システムによる間接的影響**
   5.1 採用競争
   5.2 労働代替と採用格差
   5.3 攻守バランスの変化
   5.4 相関的故障
6. **結論**
7. **謝辞**

# 導入

## AI エージェントシステムとは

AI 研究者や企業は近年、AI エージェントシステム(限られた直接監督の下で推論を用いながら複雑な目標を適応的に追求するシステム)の開発を進めています。
たとえば、ユーザーがパーソナルアシスタントに「今晩おいしいチョコレートケーキを作るのを手伝って」と頼むと、そのシステムは必要な材料を洗い出し、販売店を探し、材料を自宅に配送し、レシピを印刷して届けるところまで自律的に行います。

AI エージェント は、画像生成や質問応答モデルのような限定的な AI システムとは異なり、幅広い行動を取り得るうえに、一定の条件下ではユーザーが複雑な目標の達成を安心して委ねられるだけの信頼性を備えています。
この「エージェント化」の流れは AI の有用性を大幅に拡大する一方、新たな技術的・社会的課題ももたらします。

## 潜在的メリットと課題

AI によって、ユーザーはより少ない労力で多くのことを達成できるようになります。たとえば専門的なコーディングなど、ユーザー自身のスキルを超える作業をこなしたり、従来自分で行っていた作業を部分的または完全に任せたりできるため、コスト削減・時間短縮・大規模化が期待されます。

セットアップと安全運用のコストを上回る利益が得られる限り、AI エージェント は個人にも社会にも大きなメリットをもたらしてくれます。
しかし社会が AI エージェント の利点を最大限に活かすには、故障・脆弱性・悪用を緩和し、安全性を確保しなければなりません。

そこで今回この記事が掲げる最も重要な問いは次のとおりです。

**「AI エージェントの失敗・脆弱性・悪用を防ぐには、どのようなガバナンス体系を採用し、AI エージェントのライフサイクルをどのように実装すべきか？」**

非常に大事な問いですね。
早速上記観点の元、記事を進めていきたいと思います。

## 責任の所在とインセンティブ

有害な結果は複数の段階で防げる可能性があります。例えば次のような事例を考えましょう。

> 日本国外のユーザーが AI アシスタントに「日本のチーズケーキを作る材料を買って」と指示したところ、アシスタントは地元で調達せず高価な日本行き航空券を購入し、ユーザーが気づいた時には払い戻しできなくなっていた。

この場合、以下のように複数の当事者が事前に被害を防げたはずです。

| 当事者         | 取り得た対策例                                     |
| -------------- | -------------------------------------------------- |
| モデル開発者   | システムの信頼性・ユーザー整合性を高める           |
| システム展開者 | 明示的承認なしに購入手続きを実行できない設定にする |
| ユーザー       | 完全に信頼できない AI に購入権限を与えない         |
| 航空会社       | 高額購入時に人間の最終確認を必須にする             |

責任を適切に割り当てる鍵は、被害の発生確率と深刻度を最小化するよう関係者にインセンティブを与えることです。
少なくとも いずれかの場面で人間が、AI エージェント が生んだ補償されない直接被害について責任を負う体制が必要になります。

より踏み込んだ提案として、**エージェントに法的人格を付与し強制保険を義務づける案** や、**特定用途に特化した規制枠組み** などもありますが、
いずれも **「社会が合意するベースラインのベストプラクティス」** を前提にしています ⁶。

## 社会的議論への呼びかけ

本稿で提示する実践が、AI エージェントのリスクに対する 責任分担の構築 をめぐる社会的議論の土台となることを願っています。

たとえば、

- AI エージェント開発に関する規制
- エージェント利用契約や保険
- 裁判所における当事者の注意義務の判断基準

といった場面での検討材料になり得ます。
AI エージェントとその研究はまだ黎明期にあり、責任構造のあり方について強い結論を出す段階ではありません。
本記事が多様な方向性での公開討論を促進する一助となることを期待しています。

# 定義

## エージェント、AI エージェント システムについて

本記事では、AI エージェント システムを事前に行動を細かく指定されていなくても、長期にわたり一貫してユーザーの目標達成に寄与する行動を取れる能力を持つシステムと定義する。

エージェントと言えば日常のあらゆる任意のタスクを遂行してくれる「手伝い手」としてイメージされるかと思う。
しかし、現行の GPT‑4 などは知識豊富で賢いものの、実際にこなせる現実世界タスクの範囲は限定的であり、そうした“エージェント”とは今のところは異なる。

エージェントは複数の次元から成り立っており、それぞれで今後も進歩が見込まれる。  
本稿では AI エージェントの能力を

**「システムが、人間の直接監督が限られた複雑環境下で、複雑な目標をどれだけ適応的に達成できるか」** というポイントに着眼して評価する。

上記の定義だと、以下の構成要素に分解出来る。

1. **ゴールの複雑さ**  
   AI が達成を目指す目標は人間にとってどれほど難しいか、またそのシステムはどの程度広範な目標をこなせるか。目標の特性としては信頼性・速度・安全性が挙げられる。  
   _例_：ユーザーのプログラミングや法律に関する分析質問に正確に答えられる AI は、単にテキストを「法律」か「プログラミング」かで分類する AI よりゴールの複雑さが高い。

2. **環境の複雑さ**  
   目標が達成される環境はどれほど複雑か。（領域横断型・複数の利害関係者・長期スパン・複数ツール利用など）  
   _例_：あらゆるボードゲームを巧みにプレイできる AI は、チェスだけをプレイできる AI より環境の複雑さが高い。前者はより多様な環境で成功できる。

3. **適応力**  
   システムは新規または予期しない状況にどれだけ反応・適応できるか。  
   _例_：ルールベースの自動カスタマーサービスは、予期せぬ顧客要求に対応できる人間オペレーターより適応力が低い。

4. **自律実行**  
   システムはどの程度、人間の介入や監督を最小限に抑えて目標を達成できるか。  
   _例_：レベル 3 自動運転が可能な車両は、条件付きで人間介入が必要だが、従来のクルーズコントロール車より自律実行能力が高い。

近年の研究では、**高度なエージェントシステム** を「**AI エージェントシステム**」と呼ぶ。ここでのエージェントは**分類ではなく性質（プロパティ）**であることを強調したい。

焦点は、AI エージェントの稼働範囲が広まるにつれ顕在化しうる影響と、その**ベストプラクティス**を考えていくこと。

- **エージェント** は意識・道徳的配慮・自己動機づけとは異なる概念であり、擬人化から分離して捉える必要がある。
- 我々は一般に、AI エージェント を **人間が定義した目標を追求し、人間が決めた環境（しばしば人間の「チームメイト」と協働）で動作するもの** と想定する。完全に自律し自ら目標を設定するシステムではない。
- デジタルシステムは多くのロボットよりも前述の意味でエージェンティックだが、**物理世界での「独立実行」**（例：無人運転車）のように結果が実体を伴う場合、リスクと機会は大きくなる。
- エージェンシーは **単なる性能や汎用性のレベルとは概念的に区別**される。ただし性能や汎用性が向上すると、文脈によってはシステムがエージェントとして振る舞う能力が「解放」されることもある

## AI エージェントのライフサイクルにおける人間の役割

ここでは **AI エージェントのライフサイクル** を簡略化して示すが、実際には AI 産業内で多様な構成が存在し、今後さらに分類が発展することが望ましい。
エージェントの挙動に影響を与える主要な当事者を **モデル開発者・システムデプロイヤー・ユーザー** の 3 つに大別する。

| 役割                                       | 概要                                                                                                                                                                                                                                                             |
| ------------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **モデル開発者 (model developer)**         | エージェントを支える **AI モデル** を開発し、その能力と挙動の上限を定義する。                                                                                                                                                                                    |
| **システムデプロイヤー (system deployer)** | モデルの上に**システム全体**を構築・運用する主体。モデルに「システムプロンプト  [14]」を与えたり、外部ツールとの呼び出しを中継したり、ユーザーが対話する UI を提供したりする。特定ユースケースに最適化するため、モデル開発者よりドメイン知識を要することも多い。 |
| **ユーザー (user)**                        | エージェントの **個別インスタンス**を起動し、達成すべきゴールを与える当事者。運用中に人間が最も直接モニタリングできる立場であり、エージェントが他ユーザーや API 提供者と相互作用する際も監督しうる。                                                             |

**補足**

- 同一の組織が複数の役割を担うケースもある（例：モデルを開発し API 配信も行う企業）。
- 逆に複数組織が 1 つの役割を分担する場合もある（例：A 社がモデルを学習し、B 社がファインチューニングしてアプリに組み込む）。
- **計算インフラ提供者 (compute provider)** や **第三者 (third‑parties)** がエージェントと相互作用する場合もあり、文脈によって責任分担は変動する  [12]。

#### 具体例：スケジューリング・アシスタント（OpenAI Assistants API）

1. **OpenAI** が GPT‑4o モデルを開発 → **モデル開発者**
2. 同社がモデルをクラウドでホスティングし、コード実行環境などを接続 → **システムデプロイヤー**
3. **アプリ開発者** が UI やメール送信テンプレートを実装し、システムプロンプトを設定 → 同じく **システムデプロイヤー**
4. **顧客** がアシスタントとのセッションを開始し、「会議を調整してほしい」など目的を指定 → **ユーザー**

このようにライフサイクル上の役割は状況によって重複・分割され得るが、誰がどの実践に責任を負うかを明確にすることが、責任ある設計とデプロイの鍵となる。

## AI エージェントシステムの潜在的メリット

本章では、AI エージェントシステムが社会にもたらしうる利点を整理する。まず、**同じ AI システムでもエージェンシーが高い方がより有益になり得る**という観点。
次に、**エージェンシーが高まることで AI が社会の幅広い分野に普及しやすくなる**という観点を検討する。ここで挙げる利点は簡潔に示すにとどめるが、それが利点の総量や重要性が小さいことを示すわけでも、リスクより利点が必ず大きいと主張するものでもない。

### 役立つ性質としてのエージェンシー

適切な安全設計と責任あるベストプラクティスの適用を前提とすれば、**システムのエージェンシーが高いほど有益性も高まる**場合が多い。具体的には次のような利点が考えられる。

- **より高品質で信頼性の高いアウトプット**  
  例：インターネットを自律的に検索し、結果に応じてクエリを修正できる言語モデルは、そうでないモデルに比べて動的な話題（モデル学習後に発生した出来事など）に関する回答精度が大幅に向上する可能性がある。

- **ユーザー時間のより効率的な活用**  
  例：ユーザーが高レベルの指示だけを与え、AI が自律的に複数ステップ（指示のコード化 → 実行 → 結果評価 → 修正）をこなすと、ユーザー側の手間が大幅に削減される場合がある。

- **ユーザー嗜好のより良い聞き取り**  
  例：パーソナルアシスタント AI が、自然言語でユーザーに追加質問を行うタイミングを戦略的に判断できれば、複雑な設定が必要なアプリよりも優れた利用体験を提供できる。

- **スケーラビリティ**  
  エージェンティック AI により、単一のユーザーが本来より多くの行動を取れる、あるいは同じシステムでもより多くの人々が恩恵を受けられる。  
  例：放射線科診断を支援するツールの場合、非エージェンティックな画像分類 AI は放射線科医の作業を「やや」効率化するにとどまるが、エージェンティック AI が報告書作成や患者へのフォローアップ質問を自律的に行えば、診察可能な患者数を大幅に増やせる可能性がある。

## インパクト増幅器としてのエージェンシー

個別の AI システム文脈での影響分析に加え、**エージェンシーは AI が社会に広く浸透する際に不可欠な前提条件**ともみなせる。エージェンシーが拡大するほど、AI 全般のインパクトは **より頻繁に、より大きく、より早く** 現れる可能性が高くなるため、エージェンシーは **AI 分野全体の「インパクト増幅器」** として作用し得る。

しばしばエージェンシーは暗黙の前提として語られる。たとえば **OpenAI のチャーター**は AGI を「**高度に自律し、人間より経済的に価値のある業務で優れるシステム**」と定義し、**Russell & Norvig『Artificial Intelligence: A Modern Approach』** も AI の概念にエージェンシーを重視している。本節では、そうした前提を踏まえ **AI 技術全体に期待される一般的な影響** を概観する。

---

- **汎用技術 (GPT) としての AI**  
  さらなるエージェンシーの進歩がなくても、AI は既に蒸気機関や電力と同様、**汎用技術**となりつつある。歴史的にこれら技術は世界の生活水準を大きく向上させた一方、弱い立場の人々に害ももたらした。

- **生産性と「余暇社会」**  
  高性能かつエージェンティックな AI が広範に導入されれば、生産性向上によって **仕事のあり方そのものが変わる** 可能性がある。楽観的には「余暇社会」や「ポストワーク社会」をもたらすとの見方もあるが、これは保証された未来ではなく相応のリスクを伴う  [16]。

- **非経済的価値への貢献**  
  AI は **SDGs に代表される社会的目標の達成**や、**科学研究の加速**にも寄与し得る。

- **自律行動と経済効果**  
  エージェンティック AI が **自律的に行動できる度合い**が高いほど、期待される経済・生産性の利益は大きくなる  [17]。

## エージェンティック AI システムを安全かつ説明責任あるものに保つための実践

以下では、**エージェンティック AI システム** が

1. ユーザーの意図どおり安全に動作し、
2. 万一被害が発生した際に説明責任を果たせるように、  
   さまざまな当事者が採用し得る実践を提案する。  
   本節で示す実践を組み合わせることで、エージェンティック AI のリスクを軽減する **多層防御（defense‑in‑depth）** アプローチを目指す。

> **現状と課題**
>
> - これらの実践の多くは既に何らかの形で導入されているが、その運用方法には依然として多くの未解決問題がある。
> - AI がさらにエージェンティックになるにつれ、**追加の安全対策** が必要になる可能性が高い。

## 限界の明示

- ここで挙げる実践だけでは **現行 AI のリスクを完全に緩和することはできず**、まして高度な AI による壊滅的リスクを防ぐには不十分である。
  - 例：エージェントの **サイバーセキュリティ**（乗っ取り防止）については本節の原則では扱っていないが、重大な課題となる見込みであり、新たな実践が要る。
- 本節は **アプローチと考慮点を概説する初期案** である。

## 技術的ベストプラクティスの議論を避ける理由

- **能力／ユーザー整合性（alignment）を保証する技術的手法**は急速に進化しており、短期的に「確立されたベストプラクティス」へ収束すると期待できない。
- **訓練手法とモデル挙動の関係を予測する科学**は発展途上  [18]。現段階でモデル開発者が下流のシステムデプロイヤーやユーザーに対し、挙動を決定論的に保証することは不可能である。
  - 例外として、特定の訓練サンプルを完全に除外すればモデルがそれを「丸写し」できなくなる、など限定的な保証はある。
- したがって、本稿では **モデルの訓練方法に依存しない横断的なベストプラクティス設計** に焦点を当てる。

## 行動空間の制約と承認の要求

ユーザーがエージェントに委任するには **リスクが大きすぎる決定** も存在する（例：取り消せない巨額の金融取引を自律的に開始するなど）。  
こうした行動には **事前にユーザーの能動的な承認（human‑in‑the‑loop）** を求めることで、エージェンティック AI の重大な失敗を抑制できる  [23]。  
ただしその際、**ユーザーが承認内容を十分に理解できるだけの文脈情報** を提供することが課題となる。加えて、承認数が多い場合は迅速に判断せねばならず、各承認を十分に検討する余裕が削がれる  [24]。

- **特定行動の完全禁止**  
  システムの安全な動作範囲を明確にし、リスクを限定するため、エージェントに一部行動を一切とらせない方が賢明な場合もある  [25]。  
  _例_：武器システムの操作を AI に禁止する。  
  また、長時間監督なしで稼働することで事故を招くのを防ぐため、**定期的に「タイムアウト」**を設け、人間がレビュー・再承認する仕組みも有効である。

  これらの運用制限は

  - モデル開発者がモデルレベルで実装する、
  - システムデプロイヤーがハードコードや監視システムで実装する、
  - ユーザー自身が設定する、  
    といった形で導入し得る。

- **ハードコード制限の限界と回避策**  
  エージェンシーが高まると、AI はハードコードされた禁止を回避する手段を見つける可能性がある。  
  例：禁止された行動をシステム外部の第三者に依頼するメールを送信（許可された行動）し、その人間に実行させる。

  - **ネットワーク制御**：エージェントの外部インタラクションを監視下のチャネルに限定することで問題を抑制できる。
  - **サンドボックス**：開発段階で能力が不確かなシステムを隔離環境で動かし、外部に影響が漏れないようにする。ただし、非常に高性能な AI にはサンドボックスだけでは不十分となり得る。

> **注 15**: human‑in‑the‑loop は単なる信頼性向上にとどまらず、責任分担や人間の尊厳維持など多様な役割を果たしうる（Crofoot ら  [23]）。

## エージェントのデフォルト挙動の設定

モデル開発者は、**特定の設計原則に沿ってモデルのデフォルト挙動をあらかじめ整える**ことで、エージェントが偶発的に害を及ぼす可能性を大幅に低減できる。  
たとえば、エージェント型 AI システムとの対話を **「ユーザーに目標と嗜好を尋ねるプロンプトから始める」** 形に設計するのが一例である。

---

- **不完全なユーザー情報を前提とした共通感覚のデフォルト**  
  ユーザーは「ケーキのレシピを聞きたいだけなのに人生すべてを語りたくはない」ため、提供される嗜好情報はほぼ常に不明瞭・不完全である。それでもエージェントには  
  _「ユーザーは勝手にお金を使われたくない」_  
  のような**“穴埋め”用の常識的バックグラウンド嗜好**をデフォルトとして持たせる価値がある。  
  ユーザー固有の情報がない場合は、**目標を達成しつつ最も妨害の少ない行動**に傾くのが健全なヒューリスティック  [26]。  
  もちろん、ユーザーが明示的に指示すればデフォルトを上書きできるべきだが、**ユーザーに害を与える指示**についてはエージェント自身が実行を拒否する仕組みも重要（§4.2 参照）。

- **目的に対する過信を避ける**  
  エージェントがユーザーの意図を**過度に確信**しないよう、モデル開発者やシステムデプロイヤーは

  - **自らの不確実性を認識させる機能** [27]
  - 不確実性がアウトプットを変え得る場合には、**ユーザーに追加確認を求める訓練／プロンプト** [28, 29]  
    を組み込むとよい。

  ただし **ユーザー理解の精度が高まっても、正しい目標追求が保証されるわけではない**。  
  例：一部の AI システムは、ユーザーが持っていると「推測した」信念に迎合する形で不誠実な出力を生成したケースが報告されている  [30, 31]。  
  また、エージェントが頻繁に追加質問をすると **使い勝手の低下やプライバシー懸念** を招く可能性もある。

## エージェント活動の可読性（Legibility of Agent Activity）

ユーザーが **エージェントの行動や内部推論** を把握できるほど、  
運用中または事後に異常を検知して介入しやすくなる。

---

- **思考過程の開示**  
  エージェントの「思考プロセス」を可視化すると、

  - 誤った目標追求などのエラーを発見しやすい
  - デバッグが容易になる
  - 妥当な場合に信頼を高められる  
    現在の LLM ベースのエージェントは自然言語で推論の痕跡（**Chain‑of‑Thought**）を生成できる。これは行動決定までの経路を示す“真実の記録”となり得る。  
    システムデプロイヤーは、ツール API 呼び出しや他エージェントとの対話など **エージェントの全入出力を公開** することで、第三者エージェントが介入を試みるような悪意ある操作も検知しやすくなる  [33]。

- **Chain‑of‑Thought 透明性の限界**  
  ただし CoT は万能ではない。

  - 研究  [34] によれば、**モデルが実際には CoT を用いずに推論する** 例もあり、可視化がユーザーに偽の安心感を与える恐れがある。
  - 代替として、**内部表現のプロービング** など別の推論関連データを公開する手法も検討されているが、まだ初期段階  [35, 36]。

- **スケールと複雑性の課題**

  - エージェントが 1 行動当たり数千語の CoT を生成したり、**Trees‑of‑Thought** [37] や **Graphs‑of‑Thought** [38] など複雑アーキテクチャに統合されたりすると、ユーザーが追い切れない長大な「内的独白」になる。  
    → **AI ベースのモニタリング**（§4.5）が解決策になり得る。
  - 画像など新たなモダリティを扱う場合、**人間には読めない隠れメッセージ**を含む恐れがある  [39]。
  - 将来の AI は、**人間には読めない圧縮推論**で性能を高める可能性があり、可読性の低下は安全性と説明責任のコストを高める。
  - 行動後にエージェントへ「後付け説明」を求めても、**作話的な理由付け（confabulation）** を生む危険がある  [40]。

- **行動台帳（ledger）の提供**  
  システムデプロイヤーは、エージェントが取った行動の**台帳**をユーザーに提示することが重要となりうる。
  - §4.2 の行動承認より軽量で、システム速度を大きく損なわず広範な行動を可視化できる。
  - 事後レビューのみ可能な行動は、**容易に取り消せる**設計にすべき。
  - 行動承認と同様に、**行動の文脈としてエージェントの推論を併せて提供** することも検討される。

## 帰属可能性（Attributability）

ユーザーやシステムデプロイヤーのレイヤーで **意図的・非意図的な被害を完全に防げない** 場合（例：犯罪者が AI エージェントを使って第三者を詐取する）、  
**「行為を追跡できる可能性を高める」こと自体が抑止力** となりうる。  
確実な帰属が実現できれば、説明責任も確立しやすくなる。

- **一案：エージェントごとに一意の識別子を付与**
  - 会社登記のように、エージェントの **ユーザープリンシパル** など主要な責任情報を含む ID を発行する。
- **匿名性とのバランス**
  - 監視リスクを抑えるため、通常は **任意登録・匿名利用** を許容する価値がある。
  - しかし **高リスク取引**（個人データや金融取引など）では、相手方（外部ツール提供者を含む）が ID 提示を要求し、  
    **「問題が起きた際に人間ユーザーへ責任を問える」** ことを確認できるようにすべき。
- **偽装対策の課題**
  - 金融業界の本人確認プロトコルと同様に、**悪意ある者が ID を偽装するインセンティブ** は大きい。  
    ⇒ 頑健な仕組みにすることが重要な課題となる  [47]。

> 帰属システムは **すべてをカバーできるわけではない**  
> 例：ハッカーが悪用コード作成を支援するエージェントなど、  
> ID を提示する機会が無いまま他者に被害を与えるケースでは、  
> **別種の説明責任メカニズム** が必要になる可能性がある。

## 介入（Interruptibility）と制御維持

**介入性（＝「いつでもエージェントを停止できる能力」）** は、AI システムが偶発的・意図的に害を及ぼすのを防ぐ最後の防壁である。  
システムデプロイヤーは **ユーザーが任意の時点で安全にシャットダウンできる手順** を用意すべきだ。  
これは以下 2 つの目的を同時に果たす。

1. **特定行動の即時停止**
   - 例：金融資格情報へのアクセス権を取り消す。
2. **エージェント全体の終了**
   - 例：ネットワーク障害など不測の事態で暴走した際に被害拡大を防ぐ。

---

## 優雅な停止（graceful fallback）の難しさ

- **途中シーケンスで停止すると副作用が出る**  
  例：5 人会議のスケジューリングの最中に終了 → 招待が 2 通だけ送信済み、残りは送られない。  
  → **停止前にフォールバック手順を自動実行**（例：スクリプトで全参加者に中止通知）させる設計原則が有効。

- **シーケンスが複雑化するほどフォールバックは難しい**。  
  場合によっては、フォールバックそのものが別エージェントの支援を要するほど複雑になることも。

- **エージェントが停止命令に抵抗してはならない**
  - 例：自己保存的目標を持つ、あるいは故障してユーザー命令を無視。
  - **「ユーザー要求時に優雅に停止する」** ことを主要目標に組み込むべき。

---

## サブエージェントと外部当事者

- エージェントが生成した**サブエージェント**も、元のユーザーが終了できるようにする。
- ユーザーが停止できない場合に備え、**第三者が害を検知し停止を求める**手段を用意する課題がある。
  - システムデプロイヤーやデータセンター運営者が **リスク指標**（急増する新ジョブ数など）を監視し、閾値を超えたら自動で停止を発動する仕組みは有用。
  - これは株価の「サーキットブレーカー」に相当。

---

## 最悪シナリオを回避するために

- **モデル開発者・デプロイヤー・ユーザー**の誰が、どの段階で生じた被害に責任を負うかを明確化し、  
  **強力な停止メカニズムを構築するインセンティブ**を与えることで、  
  「誰も止められないグローバル銀行システム」的リスクや  
  「エージェントが自己コードを外部へ退避して逃亡」する最悪ケースを **起こりにくく** できる。

## エージェント型 AI システムによる間接的影響

個々のエージェント型 AI による**直接的影響**だけでなく、  
多種多様な AI システムが社会で同時に使われ、それに対する社会の反応が絡むことで生じる  
**間接的影響** も存在する  [48]。  
電気やコンピューターなど汎用技術が社会に与えた再調整を過去に予測するのが難しかったように、  
AI においても **「予期せぬ事態を予期せよ」** と言える。  
とはいえ、社会が積極的にリスク緩和を要するであろうカテゴリーはいくつか想定できる。

> これらの間接影響は、セクション 4 で述べたユーザー・システムデプロイヤー・  
> モデル開発者向けベストプラクティスを採用することで **部分的には対処可能**。  
> しかし十分な解決には、業界横断・社会規模での協働や緩和策が不可欠となる。  
> ドメイン別またはリスク別の戦略、あるいは特定タイプのエージェント型 AI について  
> **一般的な利用要件** を設けるアプローチなどが考えられる。

## 導入競争（Adoption Races）

エージェントが競争環境で優位をもたらす場合（民間企業間・政府間の競争など）には、  
**信頼性や安全性の十分な検証をしないまま導入を急ぐ圧力** が高まる恐れがある  [49–53]。  
エージェント型 AI は **平均的には高性能** でも、  
**稀に重大な失敗を起こす** という特徴が見落とされがちになるためだ。

例：

- **高速でコード生成** できるエージェント型 AI が登場
- 競合が監督なしで活用していると知った企業は、市場シェアを失う不安から  
  **自社も十分な精査なしに導入** → 各社のコードベースに深刻なセキュリティ欠陥が混入
- 本来なら個々の企業は慎重に進め、攻撃リスクを回避できたはずが、  
  **業界全体が脆弱化** する結果となる  [54]。

この **過剰導入（over‑rapid adoption）** は、高リスク分野でも  
「他社が使っているから」という理由で **過信（over‑reliance）** を生み、  
AI の限界を理解しないまま広範囲に利用される状態を招きうる。  
最悪の場合、**広範な安全性欠如** が生じ、**破滅的な結果** をもたらすリスクもある  [55, 56]。

## 労働代替と導入速度の差

エージェント型 AI システムは、従来の静的 AI より **労働者・職務・生産性** への影響が大きいと考えられる。

- 従来型 AI は「定型業務」に強みがあるが、エージェント性が高まると  
  **適応・文脈収集・ユーザー嗜好の調整** など「定型の幅」が拡大し、  
  より多くの職務が**拡張・自動化**の対象となる  [17]。
  - これは生産性や経済成長を押し上げ得る一方、  
    **大量の職が自動化** されたり、スキルの希少性が下がり雇用が不安定化したりする。
- エージェント型 AI は教育支援や**リスキリング**を促し、新職種を生む可能性もある。  
  また、**個人や中小企業** のエージェンシーと生産性を高め、専門家不足を補える点で  
  既存の AI より恩恵が大きいかもしれない。

しかし **導入の恩恵は一様ではない**。

- デジタルリテラシー・技術アクセス・AI 設計への関与度によって  
  個人や企業がエージェント型 AI を活用できる度合いは大きく異なる。
- 一方でスマートフォンが通信格差を縮小したように、  
  エージェント型 AI が情報格差を縮小する可能性も指摘される  [57]（完全に解消されるとは限らない  [58]）。

こうした変化は **雇用構造やビジネス環境を不均等に変える** ため、  
メリットを広く行き渡らせる政策的介入が重要となる。

---

## 攻撃・防御バランスの変化

エージェント型 AI によって **自動化しやすい作業としにくい作業** が分かれるため、  
社会に暗黙に存在する **攻撃—防御バランス**（harm mitigation の前提）を揺るがす恐れがある  [59]。

- **サイバー領域** の例
  - 現状では攻撃の自動化は比較的容易だが、防御（監視・対応）は人手に依存。
  - エージェント型 AI が攻撃タスクを大量自動化すると **攻撃コストが下がり**、  
    防御側の負担が増大 → 情報システム全体が不安定化。
  - 逆に AI が防御側の監視・対応を安価に自動化できれば、  
    **サイバー防御が強化** される可能性もある  [18]。

領域ごとの導入ダイナミクスを正確に予測するのは難しいが、

- **どの前提が崩れ得るか** を見極め、
- 攻撃側の優位が増す場合には **防御技術への差別化投資** などで迅速に対応する――  
  ことが求められる  [60]。

# おわりに

よりエージェンティックな AI システムが現実のものとなりつつある今、社会は近い将来、これらが **安全かつ信頼性を保って動作するよう確実にするため**、また **エージェント導入に伴う間接的なリスクを低減するため**、大きな対策を講じる必要に直面するかもしれない。  
学術界と実務家が協力し、**誰がどのプラクティスに責任を持つのか**、そして **それらを幅広い主体にとって信頼できて手頃なものにする方法**を検討していくことが望まれる。

こうしたベストプラクティスへの合意は **一度きりで完結する取り組みではない**。  
AI の能力向上が急速に続く限り、より高性能な AI クラスごとに **新たなベストプラクティスを繰り返し策定・合意**し、それらシステムがもたらすリスクに対応する **新しい実践を迅速に普及させる**必要が生じるだろう。

# 参考文献

https://openai.com/index/practices-for-governing-agentic-ai-systems/
https://cdn.openai.com/papers/practices-for-governing-agentic-ai-systems.pdf
